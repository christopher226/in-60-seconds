### Regularization

- Mean Target Encoding
 -Leakを生じさせてしまうのでFoldに合わせたencodingが必要
- Noise
 -noiseを加える
- Expanding Mean Target Encoding
 -カテゴリ内でその行より前までの平均をとる

---

### Hyperparameter Tuning
- Hyperparameter tuningの方法をモデル/ライブラリごとに学ぶ
- 大きくなるほどmodelを制約するもの、逆に柔軟にするものに分類できる
- 柔軟になる程overfittingが起きやすくなる、逆に制約されるほどunderfittingが起きやすくなる
- overfitting/underfittingどちらが生じているか見極めながらparameterを探査する
- hyperparameter探査を自動で行うライブラリもあるが、手作業でやることが多い

---

### GBDT models (XGboost/Lightgbm)
- min_child_weight: 重要 0,5,15,300くらいの順で試す
- eta: 小さい値(0.1か0.01)から試して、徐々に大きくしていく
 - etaをx倍、learning rateを1/xするとスコアが改善する場合が多い
- random seedをいくつか試して結果が変化しないことを確認すること

---

###  RandomForest/ExtraTrees
- N_estimators: 
 - try 10 and then 300, if you have enough N_estimators, it should be saturated
- max_depth, try 7 first, unlimited in default setting. it may 10, 20 or higher(higher than xgboost
- min_sample_leaf is corresponding to min_child_weight
- criteria: Geni is generally better than Entropy
- n_jobs: do not forget to set as num of cores

---

### DNN
-overfit to underfi
-optimization
-SGD+momentum or Ada/AdaDelta
-Batch size: Do not use too large batch size 500, leading to overfitting, 32 or 64 is appropriate to start with
-too small batchsize leads to noisy gradietns
-learning rate: try 0.1 first and decrease it untile you find the best fitted one
-x times batch size and x times learning rate at the same time
-Dropout: do not set dropout layer immediately after the data input